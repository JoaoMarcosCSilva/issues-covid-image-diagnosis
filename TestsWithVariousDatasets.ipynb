{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starts the autoreload extension, which allows editing the .py files with the notebook running and automatically imports the latest changes\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import trim_duplicates, model, network, gradcam, plots\n",
    "from dataset import Dataset\n",
    "import haiku as hk\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import jax\n",
    "import sklearn\n",
    "import wandb\n",
    "\n",
    "assert jax.local_device_count() >= 8\n",
    "\n",
    "NUM_CLASSES = 4\n",
    "SEED = 12\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "def basemodel_process(x): return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = jax.random.PRNGKey(SEED)\n",
    "dataset_mendeley = Dataset.load(\"mendeley\", rng=rng)\n",
    "dataset_tawsifur = Dataset.load(\"tawsifur\", rng=rng)\n",
    "print(\"Loaded mendeley\", dataset_mendeley.classnames)\n",
    "print(\"Loaded tawsifur\", dataset_tawsifur.classnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net, optim = model.init_net_and_optim(dataset_mendeley.x_train, NUM_CLASSES, BATCH_SIZE)\n",
    "\n",
    "# Gets functions for the model\n",
    "net_container = network.create(net, optim, BATCH_SIZE, shape = (10, 256, 256, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def five_fold_cross_validation(model_name, original_dataset, process_fn, seed=12, num_epochs=30, use_wandb=False):\n",
    "    config = { 'dataset' : original_dataset.name,\n",
    "               'random_seed' : seed,\n",
    "               'batch_size' : BATCH_SIZE,\n",
    "               'resolution' : 256 }\n",
    "\n",
    "    group = model_name + '_CV'\n",
    "\n",
    "    for i in range(5):\n",
    "        job_type = 'train_and_eval'\n",
    "        # RUN 1: Train and eval\n",
    "        if use_wandb:\n",
    "            run = wandb.init(project='xrays', entity='usp-covid-xrays',\n",
    "                            group=group,\n",
    "                            job_type=job_type,\n",
    "                            name=model_name + \"_CV_\" + str(i),\n",
    "                            reinit=True, config = config)\n",
    "\n",
    "        cv_dataset = original_dataset.five_fold(i)\n",
    "\n",
    "        trained = model.train_model(model_name + \"_CV\" + str(i),\n",
    "                          net_container, process_fn,\n",
    "                          cv_dataset, masks = None,\n",
    "                          num_epochs = num_epochs,\n",
    "                          wandb_run = run if use_wandb else None)\n",
    "\n",
    "        if use_wandb:\n",
    "            run.finish()\n",
    "\n",
    "        # RUN 2: Remove duplicates\n",
    "        if use_wandb:\n",
    "            run2 = wandb.init(project='xrays', entity='usp-covid-xrays',\n",
    "                            group=group,\n",
    "                            job_type='duplicate_removal',\n",
    "                            name='duprem_' + model_name + \"_CV_\" + str(i),\n",
    "                            reinit=True, config = config)\n",
    "        \n",
    "        sims = trim_duplicates.compute_similarities(cv_dataset, net_container, trained, pixel_space=False)\n",
    "        cv_dataset_curated = trim_duplicates.remove_duplicates(\"\", cv_dataset, sims,\n",
    "                                        wandb_run=run2 if use_wandb else None)\n",
    "        \n",
    "        pix_sims = trim_duplicates.compute_similarities(cv_dataset, net_container, trained, pixel_space=True)\n",
    "        trim_duplicates.remove_duplicates(\"_pix\", cv_dataset, pix_sims,\n",
    "                                          wandb_run=run2 if use_wandb else None)\n",
    "\n",
    "        if use_wandb:\n",
    "            run2.finish()\n",
    "\n",
    "        # RUN 3: Re-train and re-eval\n",
    "        if use_wandb:\n",
    "            run3 = wandb.init(project='xrays', entity='usp-covid-xrays',\n",
    "                            group=group,\n",
    "                            job_type=job_type + '_nodups',\n",
    "                            name='nodups_' + model_name + \"_CV_\" + str(i),\n",
    "                            reinit=True, config = config)\n",
    "\n",
    "        model.train_model('nodups_' + model_name + \"_CV\" + str(i),\n",
    "                          net_container, process_fn,\n",
    "                          cv_dataset_curated, masks = None,\n",
    "                          num_epochs = num_epochs,\n",
    "                          wandb_run = run3 if use_wandb else None)\n",
    "\n",
    "        if use_wandb:\n",
    "            run3.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert jax.local_device_count() >= 8\n",
    "\n",
    "five_fold_cross_validation(\"base_mendeley\", dataset_mendeley, basemodel_process, num_epochs=30, use_wandb=True)\n",
    "five_fold_cross_validation(\"base_tawsifur\", dataset_tawsifur, basemodel_process, num_epochs=30, use_wandb=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basemodel_tawsifur = model.train_model(model_name + \"CV\"s + str(i), net_container, process_fn, cv_dataset)\n",
    "sims = trim_duplicates.compute_similarities(dataset_tawsifur, net_container, basemodel_tawsifur)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresh = 0.998\n",
    "max_sims = sims.max(axis=1) - thresh\n",
    "y_classes = dataset_tawsifur.y_all[:sims.shape[0]].argmax(1)\n",
    "max_sims_index = sims.argmax(axis=1)\n",
    "mask = (max_sims >= 0) & (max_sims <= 0.0005)\n",
    "indices = np.where(mask)[0]\n",
    "plots.compare_images(dataset_tawsifur.x_all[indices], dataset_tawsifur.x_all[max_sims_index[indices]], rows=10)\n",
    "#trim_duplicates.plot_similarities(dataset_tawsifur, sims, threshold=0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trim_duplicates.plot_similarities(dataset_tawsifur, sims, threshold=thresh)\n",
    "dataset_tawsifur_curated = trim_duplicates.remove_duplicates(dataset_tawsifur, sims, threshold=thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset_tawsifur_curated.x_train.shape, dataset_tawsifur.x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basemodel_tawsifur_curated = model.train_model(\"basemodel_tawsifur_curated\", net_container, basemodel_process, dataset_tawsifur_curated)\n",
    "y_pred_tawsifur_curated = net_container.predict(basemodel_tawsifur_curated.params, basemodel_tawsifur_curated.state, dataset_tawsifur_curated.x_test)\n",
    "plots.confusion_matrix(dataset_tawsifur_curated, y_pred_tawsifur_curated, \"Tawsifur - Curated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer learning test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = jax.random.PRNGKey(SEED)\n",
    "dataset_mendeley = Dataset.load(\"mendeley\", rng=rng)\n",
    "\n",
    "net, optim = model.init_net_and_optim(dataset_mendeley.x_train, NUM_CLASSES, BATCH_SIZE)\n",
    "\n",
    "# Gets functions for the model\n",
    "net_container = network.create(net, optim, BATCH_SIZE, shape = (10, 256, 256, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test of tawsifur on mendeley\n",
    "\n",
    "basemodel_tawsifur = model.train_model(\"basemodel_tawsifurCV0\", net_container, basemodel_process, dataset_mendeley)\n",
    "y_test_pred = net_container.predict(basemodel_tawsifur.params, basemodel_tawsifur.state, dataset_mendeley.x_test)\n",
    "matrix = sklearn.metrics.confusion_matrix(\n",
    "        dataset_mendeley.y_test[0:y_test_pred.shape[0],].argmax(1),\n",
    "        y_test_pred.argmax(1), normalize = 'true'\n",
    "    )\n",
    "plots.heatmatrix(matrix, \"Transfer learning from tawsifur to mendeley\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
